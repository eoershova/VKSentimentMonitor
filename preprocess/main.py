import re
import emoji
import pandas as pd

pos_emojis = ['ğŸ˜€', 'ğŸ˜', 'ğŸ˜‚', 'ğŸ¤£', 'ğŸ˜ƒ', 'ğŸ˜„', 'ğŸ˜†', 'ğŸ˜Š', 'ğŸ˜‹', 'ğŸ˜',
 'ğŸ˜', 'ğŸ˜˜', 'ğŸ¥°', 'ğŸ˜š', 'â˜ºï¸', 'ğŸ™‚', 'ğŸ¤—', 'ğŸ¤©', 'ğŸ˜', 'ğŸ˜Œ', 'ğŸ˜›', 'ğŸ˜œ',
 'ğŸ˜', 'ğŸ™ƒ', 'ğŸ¤‘', 'ğŸ¤ ', 'ğŸ˜‡', 'ğŸ¥³', 'ğŸ¥º', 'ğŸ˜…', 'ğŸ¤¤', 'ğŸ¤§', 'ğŸ¥µ', 'ğŸ˜³',
  'ğŸ˜‰', 'ğŸ˜²', 'ğŸ˜¯', 'ğŸ¥³', 'ğŸ˜ˆ', 'ğŸ˜º','ğŸ˜¸', 'ğŸ˜¹', 'ğŸ˜»', 'ğŸ˜½', 'ğŸ˜¼', 'ğŸ‘', 'âœŒï¸',
  'ğŸ¤Ÿ', 'ğŸ¤˜', 'ğŸ‘Œ', ' ğŸ¤™', 'ğŸ™Œ', 'ğŸ™', 'ğŸ‘', 'ğŸ¤', ' ğŸ‘€', 'â¤ï¸', 'ğŸ§¡', 'ğŸ’›',
   'ğŸ’š', 'ğŸ’™', 'ğŸ’œ', 'ğŸ¤', 'ğŸ–¤', 'â£ï¸', 'ğŸ’•', 'ğŸ’', 'ğŸ’“', 'ğŸ’—', 'ğŸ’–', 'ğŸ’˜',
   'ğŸ’', 'ğŸ’Ÿ', 'â™¥ï¸', 'ğŸ’”', 'ğŸ’Œ', 'ğŸŠ', 'ğŸ‰', 'ğŸ”', 'ğŸ†’', 'ğŸ†—',
   'ğŸ’‘', 'ğŸ‘©â€â¤ï¸â€ğŸ‘©', 'ğŸ‘¨â€â¤ï¸â€ğŸ‘¨', 'ğŸ’', 'ğŸ‘©â€â¤ï¸â€ğŸ’‹â€ğŸ‘©', 'ğŸ‘¨â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨', 'ğŸ‘¸', 'ğŸ¤´', 'ğŸ™†â€â™€ï¸', 'ğŸ™†â€â™‚ï¸', 'ğŸ•º', 'ğŸ’ƒ', 'ğŸ™ˆ', 'ğŸ±', 'ğŸ¶', 'ğŸ”†', 'ğŸ”…', 'â˜€ï¸','ğŸ†', 'ğŸŒ…', 'ğŸŒ„', 'ğŸŒ ',
    'ğŸ‡', 'ğŸ†', 'ğŸŒ‡', 'ğŸŒŒ', 'ğŸ’¡', 'ğŸ–', 'ğŸ', 'ğŸ’¯', 'âœ…', 'âœ”ï¸', 'â˜‘ï¸', 'âœ“', 'â•', 'ğŸ', 'ğŸ’ª', 'ğŸ‘¼',
     'ğŸ€', 'ğŸŒ', 'ğŸŒ¸', 'ğŸŒ¼', 'ğŸŒ»', 'ğŸŒº', 'ğŸŒ¹', 'ğŸ’', 'ğŸŒˆ', 'ğŸˆ', 'ğŸ’¥', 'ğŸ”¥', ':)',
      'ğŸ‘‰ğŸ‘ˆ', 'ğŸ¦‹', 'ğŸ¥', 'ğŸ¾', 'â­', 'ğŸŒŸ', 'ğŸ’«', 'âœ¨', 'ğŸ›', 'ğŸ’¦', 'ğŸ‘‘',
      'ğŸ‘', 'ğŸ¥', 'ğŸº', 'ğŸ»', 'ğŸ¥‚', 'ğŸ°', 'ğŸ¥‡', 'ğŸ–', 'ğŸ…', 'ğŸ§šâ€â™€ï¸', 'ğŸ§š', 'ğŸ§šâ€â™‚ï¸', 'ğŸŒ¤']


neg_emojis = ['ğŸ¤¨', 'ğŸ˜', 'ğŸ˜‘', 'ğŸ˜¶', 'ğŸ™„', 'ğŸ˜£', 'ğŸ˜¥', 'ğŸ¤', 'ğŸ˜«', 'ğŸ˜“',
 'ğŸ˜”', 'ğŸ˜•', 'â˜¹ï¸', 'ğŸ™', 'ğŸ˜–', 'ğŸ˜', 'ğŸ˜Ÿ', 'ğŸ˜¤', 'ğŸ˜¢', 'ğŸ˜­', 'ğŸ˜¦', 'ğŸ˜§',
  'ğŸ˜¨', 'ğŸ˜©', 'ğŸ˜¬', 'ğŸ˜°', 'ğŸ˜¡', 'ğŸ˜ ', 'ğŸ¤¬', 'ğŸ¤¥', 'ğŸ¤•', 'ğŸ¤¢', 'ğŸ¤¯', 'ğŸ¤®', 'ğŸ˜ª', 'ğŸ¤•', 'ğŸ‘¿', 'ğŸ˜¾',
   'ğŸ˜¿', 'ğŸ™€', 'ğŸ‘', 'ğŸ¤¦â€â™€ï¸', 'ğŸ¤¦â€â™‚ï¸', 'ğŸ™â€â™€ï¸', 'ğŸ™â€â™‚ï¸', 'ğŸ™â€â™€ï¸', 'ğŸ™â€â™‚ï¸', 'ğŸ™…â€â™€ï¸', 'ğŸ™…â€â™‚ï¸', 'ğŸ¥€', 'ğŸŒ¥', 'ğŸŒ§',
    'â›ˆ', 'ğŸ’”', 'ğŸ’¢', 'ğŸ”´', 'ğŸš¨', 'ğŸ§¯', 'â‰ï¸', 'ğŸ¤¡', 'ğŸ’©', 'ğŸ†–', 'â˜“', 'âŒ', 'â˜’']


neutral_emojis = ['ğŸ˜´', 'ğŸ¤’', 'ğŸ¤¢', 'ğŸ˜·', 'ğŸ˜µ', 'ğŸ¦ ', 'ğŸ¥¶', 'ğŸ¥´', 'ğŸ’¤', 'ğŸ˜±', 'ğŸ¤”']


def del_emoji(text_data):
    str_emo = emoji.demojize(text_data)
    text = re.sub(':.*?:', '', str_emo)
    return re.sub('\s+', ' ', text)


def replace_all_emoji(text_data):
    str_emo = emoji.demojize(text_data)
    text = re.sub(':.*?:', 'emoji', str_emo)
    return re.sub('\s+', ' ', text)


def replace_emoji_by_class(text_data):
    tokens = text_data.split()
    new_tokens = []
    for t in tokens:
        if t in pos_emojis:
            t = 'pos_emoji'
        elif t in neg_emojis:
            t = 'neg_emoji'
        else:
            t = re.sub(':.*?:', 'neutral_emoji', emoji.demojize(t))
        new_tokens.append(t)
    return " ".join(new_tokens)

from pymorphy2 import MorphAnalyzer

morph = MorphAnalyzer()

def lemmatize(text_data: str):
  tokens = text_data.split()
  lemms = [morph.parse(t)[0].normal_form for t in tokens]
  return " ".join(lemms)


"""
usage:
lemms_string = lemmatize(text_string)
"""

import re
from urllib.request import urlretrieve
import os.path

from navec import Navec
from slovnet import NER

destination = 'navec_news_v1_1B_250K_300d_100q.tar'

if os.path.isfile(destination):
    pass
else:
    url = 'https://storage.yandexcloud.net/natasha-navec/packs/navec_news_v1_1B_250K_300d_100q.tar'
    urlretrieve(url, destination)

navec = Navec.load('navec_news_v1_1B_250K_300d_100q.tar')
ner = NER.load('slovnet_ner_news_v1.tar')
ner.navec(navec)


def delete_ner(text):
    try:
        ner_spans = ner(text).spans
        for span in ner_spans:
            span_value = text[span.start:span.stop]
            text = re.sub(span_value, ' ', text)
        text = " ".join(text.split())
    except Exception:
        pass
    return text


def replace_ner(text):
    try:
        ner_spans = ner(text).spans
        for span in ner_spans:
            span_value = text[span.start:span.stop]
            text = re.sub(span_value, ' NER ', text)
        text = " ".join(text.split())
    except Exception:
        pass
    return text

import string

def del_punct(text_data):
    tokens = text_data.split()

    # delete punctuation symbols
    tokens = [i for i in tokens if ( i not in string.punctuation )]

    return " ".join( tokens )


from nltk.corpus import stopwords

stop_words = set(stopwords.words('russian'))

badwords = [
    u'Ñ', u'Ğ°', u'Ğ´Ğ°', u'Ğ½Ğ¾', u'Ñ‚ĞµĞ±Ğµ', u'Ğ¼Ğ½Ğµ', u'Ñ‚Ñ‹', u'Ğ¸', u'Ñƒ', u'Ğ½Ğ°', u'Ñ‰Ğ°', u'Ğ°Ğ³Ğ°',
    u'Ñ‚Ğ°Ğº', u'Ñ‚Ğ°Ğ¼', u'ĞºĞ°ĞºĞ¸Ğµ', u'ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹', u'ĞºĞ°ĞºĞ°Ñ', u'Ñ‚ÑƒĞ´Ğ°', u'Ğ´Ğ°Ğ²Ğ°Ğ¹', u'ĞºĞ¾Ñ€Ğ¾Ñ‡Ğµ', u'ĞºĞ°Ğ¶ĞµÑ‚ÑÑ', u'Ğ²Ğ¾Ğ¾Ğ±Ñ‰Ğµ',
    u'Ğ½Ñƒ', u'Ñ‡ĞµÑ‚', u'Ğ½ĞµĞ°', u'ÑĞ²Ğ¾Ğ¸', u'Ğ½Ğ°ÑˆĞµ', u'Ñ…Ğ¾Ñ‚Ñ', u'Ñ‚Ğ°ĞºĞ¾Ğµ', u'Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€', u'ĞºĞ°Ñ€Ğ¾Ñ‡', u'ĞºĞ°Ğº-Ñ‚Ğ¾',
    u'Ğ½Ğ°Ğ¼', u'Ñ…Ğ¼', u'Ğ²ÑĞµĞ¼', u'Ğ´Ğ°', u'Ğ¾Ğ½Ğ¾', u'ÑĞ²Ğ¾ĞµĞ¼', u'Ğ¿Ñ€Ğ¾', u'Ğ²Ñ‹', u'Ğ¼', u'Ñ‚Ğ´',
    u'Ğ²ÑÑ', u'ĞºÑ‚Ğ¾-Ñ‚Ğ¾', u'Ñ‡Ñ‚Ğ¾-Ñ‚Ğ¾', u'Ğ²Ğ°Ğ¼', u'ÑÑ‚Ğ¾', u'ÑÑ‚Ğ°', u'ÑÑ‚Ğ¸', u'ÑÑ‚Ğ¾Ñ‚', u'Ğ¿Ñ€ÑĞ¼', u'Ğ»Ğ¸Ğ±Ğ¾', u'ĞºĞ°Ğº', u'Ğ¼Ñ‹',
    u'Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾', u'Ğ±Ğ»Ğ¸Ğ½', u'Ğ¾Ñ‡ĞµĞ½ÑŒ', u'ÑĞ°Ğ¼Ñ‹Ğµ', u'Ñ‚Ğ²Ğ¾ĞµĞ¼', u'Ğ²Ğ°ÑˆĞ°', u'ĞºÑÑ‚Ğ°Ñ‚Ğ¸', u'Ğ²Ñ€Ğ¾Ğ´Ğµ', u'Ñ‚Ğ¸Ğ¿Ğ°', u'Ğ¿Ğ¾ĞºĞ°', u'Ğ¾Ğº'
]


def del_stop_words(text_data: str, add_stop=[]):
    tokens = text_data.split()
    words = [t for t in tokens if t not in stop_words and t not in add_stop]
    return " ".join(words)


"""
usage:
import del_stop_words from drop_stopwords
string_without_stop_words = del_stop_words(text_string)
"""

import re
from pymorphy2.tokenizers import simple_word_tokenize

def tokenize(text_data):
    text_data = str(text_data).lower() # Ğº Ğ½Ğ¸Ğ¶Ğ½ĞµĞ¼Ñƒ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ñƒ
    text_data = re.sub('\s+', ' ', text_data) # Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²ÑĞµ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑ‹ Ğ¸ Ñ‚Ğ°Ğ±Ñ‹ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñƒ
    text_data = re.sub('\[id\d*\|\w*\]', 'username', text_data) # Ğ·Ğ°Ğ¼ĞµĞ½Ğ° Ğ²ÑĞµÑ… ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹
    text_data = re.sub('"', '', text_data) # ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ñ‹Ñ… ĞºĞ°Ğ²Ñ‹Ñ‡ĞµĞº
    text_data = re.sub("'", '', text_data) # ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ°Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ñ„Ğ¾Ğ²
    text_data = re.sub('Ñ‘', 'Ğµ', text_data) # Ğ·Ğ°Ğ¼ĞµĞ½Ğ° Ğ Ğ½Ğ° Ğ•
    text_data = re.sub(r'(?<=\w)\*', 'Ñ‘', text_data) # Ğ·Ğ°Ğ¼ĞµĞ½Ğ° * Ğ½Ğ° Ğ, ĞµÑĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ğ½Ğ¸Ğ¼ Ğ±ÑƒĞºĞ²Ğ°
    tokens = simple_word_tokenize(text_data) # Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    return ' '.join(tokens)


"""
usage:
from preprocess import tokenization
token_string = tokenization.tokenize(text_string)
"""

import requests

url = 'https://raw.githubusercontent.com/odaykhovskaya/obscene_words_ru/master/obscene_corpus.txt'
r = requests.get(url)
obscene_words = r.text.lower().split('\n')[:-1]


def replace_vulgar(text):
    tokens = text.split(' ')
    for i, v in enumerate(tokens):
        if 'Ñ‘' in v:
            tokens[i] = 'vulgar'
        elif v in obscene_words:
            tokens[i] = 'vulgar'
    return ' '.join(tokens)


def preprocess(text, params={'punctuation_deletion': 'yes',
                             'lemmatization': 'no',
                             'stopwords_deletion': 'no', 
                             'emojis_processing': 'no', 
                            'preprocess_ner': 'del',
                             'vulgar_processing': 'yes'}):

    print('preproc has started')
    # print('BP', text)

    text = tokenize(text)

    if params['punctuation_deletion'] == 'no':
        pass 
    elif params['punctuation_deletion'] == 'yes':
        text = del_punct(text)
    else:
        raise ValueError('Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ†Ğ¸Ğ¸ Ğ½ĞµÑ‚')
        
        
    if params['preprocess_ner'] == 'no':
        pass 
    elif params['preprocess_ner'] == 'del':
        text = delete_ner(text)
    elif params['preprocess_ner'] == 'replace':
        text = replace_ner(text)
    else:
        raise ValueError('Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ†Ğ¸Ğ¸ Ğ½ĞµÑ‚')
        
        
    if params['lemmatization'] == 'no':
        pass 
    elif params['lemmatization'] == 'yes':
        text = lemmatize(text)
    else:
        raise ValueError('Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ†Ğ¸Ğ¸ Ğ½ĞµÑ‚')
        
    if params['stopwords_deletion'] == 'no':
        pass 
    elif params['stopwords_deletion'] == 'yes':
        text = del_stop_words(text, add_stop=badwords)
    else:
        raise ValueError('Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ†Ğ¸Ğ¸ Ğ½ĞµÑ‚')
        
    
    if params['emojis_processing'] == 'no':
        pass 
    elif params['emojis_processing'] == 'del':
        text = del_emoji(text)
    elif params['emojis_processing'] == 'replace':
        text = replace_all_emoji(text)
    elif params['emojis_processing'] == 'label':
        text = replace_emoji_by_class(text)
    else:
        raise ValueError('Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ†Ğ¸Ğ¸ Ğ½ĞµÑ‚')

    if params['vulgar_processing'] == 'no':
        pass
    elif params['vulgar_processing'] == 'yes':
        text = replace_vulgar(text)
    else:
        raise ValueError('Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ†Ğ¸Ğ¸ Ğ½ĞµÑ‚')

    if re.search('[Ğ°-ÑĞ-Ğ¯Ñ‘Ğ]', text) is None and len(text.split()) < 1:
        text = 'None'
    # print('AP', text)
    return text
